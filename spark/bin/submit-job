#!/usr/bin/env bash

# Make sure temp dir is set.
YAVI_SRC_DIR=${YAVI_SRC_DIR:?Need to set YAVI_SRC_DIR to non-empty value.}
YAVI_DATA_DIR=${YAVI_DATA_DIR:?Need to set YAVI_DATA_DIR to non-empty value.}
YAVI_TEMP_DIR=${YAVI_TEMP_DIR:?Need to set YAVI_TEMP_DIR to non-empty value.}

# Check args.
if [ ! $# -ge 3 ]
  then
    >&2 echo "Wrong number of arguments supplied. Expecting job id, wikipedia id, dump date and (optionally) shell arguments."
    >&2 echo "Example:"
    >&2 echo "  $0 redirect_map enwiki 20160501"
    exit 1
fi

# Read args.
job_id=$1
wiki_id=$2
dump_date=$3
shell_args=${@:3}

# Define submit directory and log file.
current_date_time=$(date '+%Y%m%d-%Hh%Mm%Ss')
submit_dir=$YAVI_TEMP_DIR/spark/$wiki_id/$dump_date/$job_id/$current_date_time/ && mkdir -p $submit_dir
log_file=$submit_dir/log.txt && touch $log_file

# Define job file and wiki data directory.
job_file=$YAVI_SRC_DIR/spark/jobs/$job_id.scala
wiki_data_dir=$YAVI_DATA_DIR/$wiki_id/$dump_date/

# Define job entry point.
run_job=":load $job_file
runJob(\"$wiki_data_dir\", sqlContext)"

# Submit job.
(cd $submit_dir && echo "$run_job" | \
  spark-shell --packages com.databricks:spark-avro_2.10:2.0.1 "$shell_args" >> $log_file 2>&1 &)
